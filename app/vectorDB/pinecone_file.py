import pandas as pd
from openai import OpenAI
import numpy as np
import json
from typing import List, Dict, Any
import time
import datetime
from tqdm import tqdm
import os

# Pinecone ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
try:
    from pinecone import Pinecone
    PINECONE_NEW_VERSION = True
    print("ğŸ“¦ ìµœì‹  Pinecone ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°ì§€")
except ImportError:
    import pinecone
    PINECONE_NEW_VERSION = False
    print("ğŸ“¦ êµ¬ë²„ì „ Pinecone ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°ì§€")

class CSVToPineconeBuilder:
    def __init__(self, pinecone_api_key: str, openai_api_key: str):
        """
        CSV íŒŒì¼ì„ Pinecone ë²¡í„° DBë¡œ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤
        """
        self.pinecone_api_key = pinecone_api_key
        self.openai_api_key = openai_api_key

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = OpenAI(api_key=self.openai_api_key)

        # Pinecone ì´ˆê¸°í™” (ë²„ì „ë³„ ì²˜ë¦¬)
        self.pinecone_initialized = False
        
        if PINECONE_NEW_VERSION:
            # ìµœì‹  ë²„ì „ ë°©ì‹
            try:
                self.pc = Pinecone(api_key=self.pinecone_api_key)
                indexes = self.pc.list_indexes()
                print(f"âœ… Pinecone ìµœì‹  ë²„ì „ìœ¼ë¡œ ì´ˆê¸°í™” ì„±ê³µ")
                print(f"ğŸ“‹ ë°œê²¬ëœ ì¸ë±ìŠ¤: {[idx.name for idx in indexes.indexes] if hasattr(indexes, 'indexes') else []}")
                self.pinecone_initialized = True
            except Exception as e:
                print(f"âŒ ìµœì‹  ë²„ì „ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        else:
            # êµ¬ë²„ì „ ë°©ì‹
            init_methods = [
                lambda: pinecone.init(api_key=self.pinecone_api_key),
                lambda: pinecone.init(
                    api_key=self.pinecone_api_key,
                    environment="us-east1-gcp"
                ),
                lambda: pinecone.init(
                    api_key=self.pinecone_api_key,
                    environment="us-east-1"
                )
            ]
            
            for i, init_method in enumerate(init_methods):
                try:
                    init_method()
                    indexes = pinecone.list_indexes()
                    print(f"âœ… Pinecone êµ¬ë²„ì „ìœ¼ë¡œ ì´ˆê¸°í™” ì„±ê³µ (ë°©ë²• {i+1})")
                    print(f"ğŸ“‹ ë°œê²¬ëœ ì¸ë±ìŠ¤: {indexes}")
                    self.pinecone_initialized = True
                    break
                except Exception as e:
                    print(f"âš ï¸ ì´ˆê¸°í™” ë°©ë²• {i+1} ì‹¤íŒ¨: {e}")
                    continue
        
        if not self.pinecone_initialized:
            raise Exception("Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨")

        # ì„¤ì •ê°’
        self.embedding_model = "text-embedding-3-small"
        self.dimension = 1024
        self.metric = "cosine"

    def load_csv_files(self, file_paths: List[str]) -> pd.DataFrame:
        """CSV íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  í•©ì¹˜ê¸°"""
        dataframes = []
        
        for file_path in file_paths:
            if os.path.exists(file_path):
                try:
                    df = pd.read_csv(file_path, encoding='utf-8')
                    print(f"âœ… {file_path} ë¡œë“œ ì„±ê³µ: {len(df)}í–‰")
                    
                    # íŒŒì¼ëª…ì„ source ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
                    df['source_file'] = os.path.basename(file_path)
                    dataframes.append(df)
                    
                    # ì»¬ëŸ¼ ë¯¸ë¦¬ë³´ê¸°
                    print(f"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}")
                    
                except Exception as e:
                    print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    # UTF-8ì´ ì•ˆë˜ë©´ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
                    try:
                        df = pd.read_csv(file_path, encoding='cp949')
                        print(f"âœ… {file_path} ë¡œë“œ ì„±ê³µ (cp949): {len(df)}í–‰")
                        df['source_file'] = os.path.basename(file_path)
                        dataframes.append(df)
                    except Exception as e2:
                        print(f"âŒ {file_path} ì™„ì „ ì‹¤íŒ¨: {e2}")
            else:
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_path}")
        
        if dataframes:
            # ëª¨ë“  DataFrame í•©ì¹˜ê¸°
            combined_df = pd.concat(dataframes, ignore_index=True)
            print(f"ğŸ”— ì´ {len(combined_df)}ê°œ ë ˆì½”ë“œ ê²°í•© ì™„ë£Œ")
            
            # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            print("\nğŸ“„ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
            print(combined_df.head())
            print(f"\nğŸ“Š ì»¬ëŸ¼ ì •ë³´:")
            print(combined_df.info())
            
            return combined_df
        else:
            print("âŒ ë¡œë“œëœ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

    def create_embedding_text(self, row: pd.Series) -> str:
        """DataFrame í–‰ì„ ì„ë² ë”©í•  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = []
        
        # ì£¼ìš” í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë“¤ì„ ì°¾ì•„ì„œ ê²°í•©
        # ì¼ë°˜ì ì¸ ì»¬ëŸ¼ëª…ë“¤ í™•ì¸
        text_columns = []
        
        for col in row.index:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in [
                'task', 'work', 'content', 'description', 'summary', 
                'ì—…ë¬´', 'ë‚´ìš©', 'ì„¤ëª…', 'ìš”ì•½', 'done', 'activity',
                'project', 'report', 'result'
            ]):
                if pd.notna(row[col]) and str(row[col]).strip():
                    text_columns.append(col)
        
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©
        for col in text_columns:
            if pd.notna(row[col]):
                text_parts.append(f"{col}: {row[col]}")
        
        # ë‹¤ë¥¸ ì¤‘ìš”í•œ ì •ë³´ë“¤ë„ ì¶”ê°€
        for col in row.index:
            if col not in text_columns and pd.notna(row[col]):
                col_lower = col.lower()
                if any(keyword in col_lower for keyword in [
                    'user', 'id', 'date', 'year', 'quarter', 'organization',
                    'ì‚¬ìš©ì', 'ë‚ ì§œ', 'ì—°ë„', 'ë¶„ê¸°', 'ì¡°ì§'
                ]):
                    text_parts.append(f"{col}: {row[col]}")
        
        # ì†ŒìŠ¤ íŒŒì¼ ì •ë³´ ì¶”ê°€
        if 'source_file' in row.index:
            text_parts.append(f"ì¶œì²˜: {row['source_file']}")
        
        return " | ".join(text_parts)

    def get_embedding(self, text: str) -> List[float]:
        """OpenAI APIë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=text,
                dimensions=self.dimension
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def create_new_index(self) -> bool:
        """ìƒˆë¡œìš´ Pinecone ì¸ë±ìŠ¤ ìƒì„± (ë²„ì „ë³„ ì²˜ë¦¬)"""
        try:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ëª©ë¡ í™•ì¸
            if PINECONE_NEW_VERSION:
                indexes_response = self.pc.list_indexes()
                existing_indexes = [idx.name for idx in indexes_response.indexes] if hasattr(indexes_response, 'indexes') else []
            else:
                existing_indexes = pinecone.list_indexes()
            
            print(f"ğŸ“‹ ê¸°ì¡´ ì¸ë±ìŠ¤: {existing_indexes}")
            
            # ìƒˆ ì¸ë±ìŠ¤ëª… ìƒì„±
            now_str = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            self.index_name = f"skore-{now_str}"
            
            print(f"ğŸ”¨ ìƒˆ ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì¤‘...")
            
            try:
                if PINECONE_NEW_VERSION:
                    # ìµœì‹  ë²„ì „ ë°©ì‹
                    self.pc.create_index(
                        name=self.index_name,
                        dimension=self.dimension,
                        metric=self.metric,
                        spec={
                            'serverless': {
                                'cloud': 'aws',
                                'region': 'us-east-1'
                            }
                        }
                    )
                else:
                    # êµ¬ë²„ì „ ë°©ì‹
                    pinecone.create_index(
                        name=self.index_name,
                        dimension=self.dimension,
                        metric=self.metric
                    )
                
                print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì„±ê³µ!")
                
                # ì´ˆê¸°í™” ëŒ€ê¸°
                print("â³ ì¸ë±ìŠ¤ ì´ˆê¸°í™” ëŒ€ê¸° ì¤‘... (60ì´ˆ)")
                time.sleep(60)
                
                # ì¸ë±ìŠ¤ ì—°ê²°
                if PINECONE_NEW_VERSION:
                    self.index = self.pc.Index(self.index_name)
                else:
                    self.index = pinecone.Index(self.index_name)
                
                # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •
                self.namespace = f"weekly_cloud/esg-{now_str}"
                print(f"ğŸ“› ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {self.namespace}")
                
                return True
                
            except Exception as create_error:
                print(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {create_error}")
                
                # pod ì œí•œ ì—ëŸ¬ì¸ ê²½ìš°
                if "max pods" in str(create_error).lower():
                    print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
                    print("1. ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ")
                    print("2. ìœ ë£Œ í”Œëœ ì—…ê·¸ë ˆì´ë“œ")
                    print("3. Chroma ì‚¬ìš©")
                    
                    if existing_indexes:
                        print(f"\nğŸ—‘ï¸ ì‚­ì œ ê°€ëŠ¥í•œ ì¸ë±ìŠ¤:")
                        for i, idx in enumerate(existing_indexes):
                            print(f"  {i+1}. {idx}")
                        
                        choice = input("ì‚­ì œí•  ì¸ë±ìŠ¤ ë²ˆí˜¸ (Enter: ê±´ë„ˆë›°ê¸°): ").strip()
                        
                        if choice.isdigit() and 1 <= int(choice) <= len(existing_indexes):
                            idx_to_delete = existing_indexes[int(choice)-1]
                            try:
                                if PINECONE_NEW_VERSION:
                                    self.pc.delete_index(idx_to_delete)
                                else:
                                    pinecone.delete_index(idx_to_delete)
                                
                                print(f"âœ… '{idx_to_delete}' ì‚­ì œ ì™„ë£Œ")
                                
                                # ì¬ì‹œë„
                                time.sleep(10)
                                
                                if PINECONE_NEW_VERSION:
                                    self.pc.create_index(
                                        name=self.index_name,
                                        dimension=self.dimension,
                                        metric=self.metric,
                                        spec={
                                            'serverless': {
                                                'cloud': 'aws',
                                                'region': 'us-east-1'
                                            }
                                        }
                                    )
                                    self.index = self.pc.Index(self.index_name)
                                else:
                                    pinecone.create_index(
                                        name=self.index_name,
                                        dimension=self.dimension,
                                        metric=self.metric
                                    )
                                    self.index = pinecone.Index(self.index_name)
                                
                                print(f"âœ… ì¬ìƒì„± ì„±ê³µ!")
                                time.sleep(60)
                                self.namespace = f"weekly_cloud/esg-{now_str}"
                                return True
                                
                            except Exception as delete_error:
                                print(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {delete_error}")
                
                return False
                
        except Exception as e:
            print(f"âŒ ì „ì²´ ê³¼ì • ì‹¤íŒ¨: {e}")
            return False

    def process_and_upload(self, df: pd.DataFrame, batch_size: int = 10):
        """DataFrame ì²˜ë¦¬ ë° Pinecone ì—…ë¡œë“œ"""
        print(f"ğŸ”„ {len(df)}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì‹œì‘...")
        
        # ì²˜ë¦¬í•  ë°ì´í„° ë²”ìœ„ ì„ íƒ
        print("\nì²˜ë¦¬ ì˜µì…˜:")
        print("1. í…ŒìŠ¤íŠ¸ (50ê°œ)")
        print("2. ì¤‘ê°„ (200ê°œ)")
        print("3. ì „ì²´ ë°ì´í„°")
        print("4. ì‚¬ìš©ì ì •ì˜")
        
        choice = input("ì„ íƒ (1-4, ê¸°ë³¸ê°’: 1): ").strip()
        
        if choice == "2":
            df_subset = df.head(200)
        elif choice == "3":
            df_subset = df
        elif choice == "4":
            limit = input("ì²˜ë¦¬í•  ê°œìˆ˜ ì…ë ¥: ").strip()
            limit = int(limit) if limit.isdigit() else 50
            df_subset = df.head(limit)
        else:
            df_subset = df.head(50)
        
        print(f"ğŸ“Š {len(df_subset)}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì˜ˆì •")
        
        vectors = []
        successful_uploads = 0
        failed_count = 0
        
        for idx, row in tqdm(df_subset.iterrows(), total=len(df_subset), desc="ì²˜ë¦¬ ì¤‘"):
            try:
                # í…ìŠ¤íŠ¸ ìƒì„±
                text = self.create_embedding_text(row)
                if not text.strip():
                    failed_count += 1
                    continue
                
                # ì„ë² ë”© ìƒì„±
                embedding = self.get_embedding(text)
                if embedding is None:
                    failed_count += 1
                    continue
                
                # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                metadata = {
                    'text': text[:800],  # í¬ê¸° ì œí•œ
                    'row_index': str(idx),
                    'source_file': str(row.get('source_file', 'unknown'))
                }
                
                # DataFrameì˜ ì£¼ìš” ì»¬ëŸ¼ë“¤ì„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                for col in row.index:
                    if pd.notna(row[col]) and col != 'source_file':
                        # ë©”íƒ€ë°ì´í„° í‚¤ ì •ë¦¬
                        clean_key = str(col).replace(' ', '_').lower()
                        metadata[clean_key] = str(row[col])[:100]  # ê¸¸ì´ ì œí•œ
                
                # ë²¡í„° ë°ì´í„° ì¤€ë¹„
                vector_data = {
                    'id': f"skore-{idx}",
                    'values': embedding,
                    'metadata': metadata
                }
                
                vectors.append(vector_data)
                
                # ë°°ì¹˜ ì—…ë¡œë“œ
                if len(vectors) >= batch_size:
                    try:
                        self.index.upsert(vectors=vectors, namespace=self.namespace)
                        successful_uploads += len(vectors)
                        print(f"âœ… {successful_uploads}ê°œ ì—…ë¡œë“œ ì™„ë£Œ")
                        vectors = []
                        time.sleep(1)
                    except Exception as e:
                        print(f"âŒ ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                        failed_count += len(vectors)
                        vectors = []
                
            except Exception as e:
                print(f"âŒ ë ˆì½”ë“œ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                failed_count += 1
                continue
        
        # ë‚¨ì€ ë²¡í„° ì—…ë¡œë“œ
        if vectors:
            try:
                self.index.upsert(vectors=vectors, namespace=self.namespace)
                successful_uploads += len(vectors)
                print(f"âœ… ìµœì¢… {len(vectors)}ê°œ ì—…ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ ìµœì¢… ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                failed_count += len(vectors)
        
        print(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"  âœ… ì„±ê³µ: {successful_uploads}ê°œ")
        print(f"  âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
        
        return successful_uploads

    def get_stats(self):
        """ì¸ë±ìŠ¤ í†µê³„ í™•ì¸"""
        try:
            stats = self.index.describe_index_stats()
            print("ğŸ“Š ìµœì¢… í†µê³„:")
            print(f"  - ì¸ë±ìŠ¤ëª…: {self.index_name}")
            print(f"  - ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {self.namespace}")
            print(f"  - ì´ ë²¡í„° ìˆ˜: {stats.total_vector_count}")
            print(f"  - ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ í†µê³„:")
            for ns_name, ns_stats in stats.namespaces.items():
                print(f"    â€¢ {ns_name}: {ns_stats.vector_count}ê°œ")
            return stats
        except Exception as e:
            print(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def run_pipeline(self, csv_files: List[str]):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ CSV to Pinecone ë²¡í„° DB êµ¬ì¶• ì‹œì‘")
        print("="*60)
        
        # 1. CSV íŒŒì¼ ë¡œë“œ
        df = self.load_csv_files(csv_files)
        if df.empty:
            return False, None, None
        
        # 2. ì¸ë±ìŠ¤ ìƒì„±
        if not self.create_new_index():
            return False, None, None
        
        # 3. ë°ì´í„° ì²˜ë¦¬ ë° ì—…ë¡œë“œ
        uploaded_count = self.process_and_upload(df)
        
        # 4. í†µê³„ í™•ì¸
        self.get_stats()
        
        if uploaded_count > 0:
            print("âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
            return True, self.index_name, self.namespace
        else:
            return False, None, None

# ê²€ìƒ‰ í•¨ìˆ˜
def search_csv_data(query: str, pinecone_api_key: str, openai_api_key: str, 
                   index_name: str, namespace: str, top_k: int = 5):
    """CSV ë°ì´í„°ì—ì„œ ê²€ìƒ‰"""
    try:
        print(f"ğŸ” ê²€ìƒ‰: '{query}'")
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        client = OpenAI(api_key=openai_api_key)
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=query,
            dimensions=1024
        )
        query_embedding = response.data[0].embedding
        
        # Pinecone ê²€ìƒ‰ (ë²„ì „ë³„ ì²˜ë¦¬)
        if PINECONE_NEW_VERSION:
            pc = Pinecone(api_key=pinecone_api_key)
            index = pc.Index(index_name)
        else:
            pinecone.init(api_key=pinecone_api_key)
            index = pinecone.Index(index_name)
        
        results = index.query(
            vector=query_embedding,
            top_k=top_k,
            namespace=namespace,
            include_metadata=True
        )
        
        print(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ({len(results.matches)}ê°œ):")
        print("-" * 60)
        
        for i, match in enumerate(results.matches, 1):
            print(f"\n{i}. ì ìˆ˜: {match.score:.4f}")
            print(f"   ID: {match.id}")
            if match.metadata:
                print(f"   ì¶œì²˜: {match.metadata.get('source_file', 'N/A')}")
                print(f"   ë‚´ìš©: {match.metadata.get('text', 'N/A')[:200]}...")
                
                # ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„°ë„ í‘œì‹œ
                for key, value in match.metadata.items():
                    if key not in ['text', 'source_file', 'row_index'] and value:
                        print(f"   {key}: {value}")
            
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

# ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ“Š CSV íŒŒì¼ë¡œ Pinecone ë²¡í„° DB êµ¬ì¶•")
    print("="*60)
    
    # API í‚¤ ì„¤ì •
    PINECONE_API_KEY = "pcsk_5Wcu2A_7QAdTAjfmSYxwxc2sfiZ7G1bhmi9qy6J2KXL1hUfNcoLQ3xGdavA7S4E9DEqpmH"
    OPENAI_API_KEY = "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"
    
    # CSV íŒŒì¼ ê²½ë¡œ
    CSV_FILES = [
        "D:/Github/ai-server/app/vectorDB/weekly_report_cloud3.csv",
        "D:/Github/ai-server/app/vectorDB/weekly_report_esg.csv"
    ]
    
    print(f"ğŸ“ ì²˜ë¦¬í•  CSV íŒŒì¼:")
    for file_path in CSV_FILES:
        print(f"  - {file_path}")
    print("="*60)
    
    # ë²¡í„° DB êµ¬ì¶• ì‹¤í–‰
    builder = CSVToPineconeBuilder(
        pinecone_api_key=PINECONE_API_KEY,
        openai_api_key=OPENAI_API_KEY
    )
    
    success, index_name, namespace = builder.run_pipeline(CSV_FILES)
    
    if success:
        print("\n" + "="*60)
        print("ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        print("="*60)
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_queries = ["í”„ë¡œì íŠ¸", "ê°œë°œ", "í´ë¼ìš°ë“œ", "ESG"]
        
        for query in test_queries:
            print(f"\n{'='*20} '{query}' ê²€ìƒ‰ {'='*20}")
            search_csv_data(
                query, 
                PINECONE_API_KEY, 
                OPENAI_API_KEY,
                index_name,
                namespace,
                top_k=3
            )
        
        print(f"\nğŸ‰ ì™„ë£Œ!")
        print(f"ğŸ“ ì¸ë±ìŠ¤: {index_name}")
        print(f"ğŸ“› ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {namespace}")
        print("ğŸ’¡ search_csv_data í•¨ìˆ˜ë¡œ ê²€ìƒ‰ ê°€ëŠ¥!")
    else:
        print("\nâŒ ë²¡í„° DB êµ¬ì¶• ì‹¤íŒ¨")
        print("ğŸ’¡ Pinecone ì œí•œ ë˜ëŠ” CSV íŒŒì¼ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")