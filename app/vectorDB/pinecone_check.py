#!/usr/bin/env python3
"""
ì™„ì „í•œ Pinecone ë°ì´í„° ë¶„ì„ê¸°
ëª¨ë“  ë²¡í„° ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ë¶„ì„í•˜ëŠ” ë„êµ¬
"""

import json
from pinecone import Pinecone
from typing import Dict, List, Any, Optional
import pandas as pd
from datetime import datetime
import time

class CompletePineconeAnalyzer:
    def __init__(self, api_key: str, index_name: str = "skore-20250624-144422"):
        """
        ì™„ì „í•œ Pinecone ë°ì´í„° ë¶„ì„ê¸° ì´ˆê¸°í™”
        """
        self.pc = Pinecone(api_key=api_key)
        self.index_name = index_name
        self.index = self.pc.Index(index_name)
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìë™ ê°ì§€
        try:
            stats = self.index.describe_index_stats()
            available_namespaces = list(stats.namespaces.keys()) if stats.namespaces else [""]
            self.namespace = available_namespaces[0] if available_namespaces and available_namespaces[0] != "" else ""
            print(f"ğŸ”Œ Pinecone ì—°ê²° ì™„ë£Œ - ì¸ë±ìŠ¤: {index_name}")
            print(f"ğŸ¯ ì‚¬ìš©í•  ë„¤ì„ìŠ¤í˜ì´ìŠ¤: '{self.namespace}'")
        except Exception as e:
            print(f"âš ï¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {e}")
            self.namespace = ""
    
    def get_complete_data_analysis(self) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„°ë¥¼ ì™„ì „íˆ ë¶„ì„í•©ë‹ˆë‹¤."""
        print(f"\nğŸ” === ì™„ì „í•œ ë°ì´í„° ë¶„ì„ ì‹œì‘ ===")
        
        # 1. ì¸ë±ìŠ¤ í†µê³„
        stats = self.index.describe_index_stats()
        total_vectors = stats.total_vector_count
        print(f"ğŸ“Š ì´ ë²¡í„° ìˆ˜: {total_vectors}")
        
        # 2. ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ (ì—¬ëŸ¬ ë²ˆ ì¿¼ë¦¬)
        all_data = self.collect_all_vectors(total_vectors)
        print(f"âœ… ì‹¤ì œ ìˆ˜ì§‘ëœ ë²¡í„° ìˆ˜: {len(all_data)}")
        
        # 3. ë©”íƒ€ë°ì´í„° ë¶„ì„
        analysis = self.analyze_complete_metadata(all_data)
        
        return analysis
    
    def collect_all_vectors(self, total_count: int, batch_size: int = 1000) -> List[Dict[str, Any]]:
        """ëª¨ë“  ë²¡í„°ë¥¼ ë°°ì¹˜ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“¦ ì „ì²´ {total_count}ê°œ ë²¡í„° ìˆ˜ì§‘ ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        all_vectors = []
        collected_ids = set()
        dummy_vector = [0.0] * 1024
        
        # ì—¬ëŸ¬ ë²ˆ ì¿¼ë¦¬í•˜ì—¬ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘
        iterations = 0
        max_iterations = 10  # ë¬´í•œë£¨í”„ ë°©ì§€
        
        while len(all_vectors) < total_count and iterations < max_iterations:
            iterations += 1
            print(f"ğŸ”„ {iterations}ì°¨ ìˆ˜ì§‘ ì¤‘... (í˜„ì¬: {len(all_vectors)}/{total_count})")
            
            try:
                query_params = {
                    "vector": dummy_vector,
                    "top_k": min(batch_size, 10000),  # Pinecone ìµœëŒ€ ì œí•œ
                    "include_metadata": True
                }
                
                if self.namespace:
                    query_params["namespace"] = self.namespace
                
                results = self.index.query(**query_params)
                
                new_vectors = 0
                for match in results.matches:
                    if match.id not in collected_ids:
                        all_vectors.append({
                            "id": match.id,
                            "score": match.score,
                            "metadata": match.metadata
                        })
                        collected_ids.add(match.id)
                        new_vectors += 1
                
                print(f"   ìƒˆë¡œ ìˆ˜ì§‘ëœ ë²¡í„°: {new_vectors}ê°œ")
                
                if new_vectors == 0:
                    print(f"   ë” ì´ìƒ ìƒˆë¡œìš´ ë²¡í„°ê°€ ì—†ìŒ, ìˆ˜ì§‘ ì™„ë£Œ")
                    break
                
                # API ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ë°©ì§€
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ {iterations}ì°¨ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                break
        
        return all_vectors
    
    def analyze_complete_metadata(self, all_data: List[Dict]) -> Dict[str, Any]:
        """ëª¨ë“  ë©”íƒ€ë°ì´í„°ë¥¼ ì™„ì „íˆ ë¶„ì„í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„° ì™„ì „ ë¶„ì„ ì¤‘... ({len(all_data)}ê°œ ë²¡í„°)")
        
        if not all_data:
            return {}
        
        # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        all_keys = set()
        user_ids = set()
        organizations = set()
        source_files = set()
        years = set()
        quarters = set()
        
        # ìƒ˜í”Œ ë°ì´í„° ì €ì¥
        sample_records = []
        
        for item in all_data:
            metadata = item.get("metadata", {})
            all_keys.update(metadata.keys())
            
            # ê° í•„ë“œë³„ ê³ ìœ ê°’ ìˆ˜ì§‘
            if "user_id" in metadata:
                user_ids.add(str(metadata["user_id"]))
            if "organization_id" in metadata:
                organizations.add(str(metadata["organization_id"]))
            if "source_file" in metadata:
                source_files.add(str(metadata["source_file"]))
            if "evaluation_year" in metadata:
                years.add(str(metadata["evaluation_year"]))
            if "evaluation_quarter" in metadata:
                quarters.add(str(metadata["evaluation_quarter"]))
            
            # ìƒ˜í”Œ ë ˆì½”ë“œ ì €ì¥ (ê° user_idë³„ë¡œ 1ê°œì”©)
            if len(sample_records) < 20:
                sample_records.append({
                    "id": item["id"],
                    "user_id": metadata.get("user_id"),
                    "organization_id": metadata.get("organization_id"), 
                    "start_date": metadata.get("start_date"),
                    "end_date": metadata.get("end_date"),
                    "done_task_preview": metadata.get("done_task", "")[:150] + "..."
                })
        
        analysis = {
            "total_vectors": len(all_data),
            "metadata_keys": sorted(list(all_keys)),
            "unique_user_ids": sorted(list(user_ids)),
            "unique_organizations": sorted(list(organizations)),
            "source_files": sorted(list(source_files)),
            "evaluation_years": sorted(list(years)),
            "evaluation_quarters": sorted(list(quarters)),
            "sample_records": sample_records
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“‹ === ì™„ì „í•œ ë¶„ì„ ê²°ê³¼ ===")
        print(f"ì´ ë²¡í„° ìˆ˜: {analysis['total_vectors']}")
        print(f"ë©”íƒ€ë°ì´í„° í‚¤: {len(analysis['metadata_keys'])}ê°œ")
        print(f"  {analysis['metadata_keys']}")
        print(f"\nğŸ‘¥ ì‚¬ìš©ì ID: {len(analysis['unique_user_ids'])}ëª…")
        print(f"  {analysis['unique_user_ids']}")
        print(f"\nğŸ¢ ì¡°ì§ ID: {len(analysis['unique_organizations'])}ê°œ")
        print(f"  {analysis['unique_organizations']}")
        print(f"\nğŸ“ ì†ŒìŠ¤ íŒŒì¼: {len(analysis['source_files'])}ê°œ")
        print(f"  {analysis['source_files']}")
        print(f"\nğŸ“… í‰ê°€ ë…„ë„: {analysis['evaluation_years']}")
        print(f"ğŸ“… í‰ê°€ ë¶„ê¸°: {analysis['evaluation_quarters']}")
        
        # ê° ì‚¬ìš©ìë³„ ë°ì´í„° ê°œìˆ˜ ê³„ì‚°
        user_data_counts = {}
        for item in all_data:
            user_id = item.get("metadata", {}).get("user_id")
            if user_id:
                user_data_counts[str(user_id)] = user_data_counts.get(str(user_id), 0) + 1
        
        print(f"\nğŸ“Š ì‚¬ìš©ìë³„ ë°ì´í„° ê°œìˆ˜:")
        for user_id in sorted(user_data_counts.keys()):
            print(f"  User {user_id}: {user_data_counts[user_id]}ê±´")
        
        analysis["user_data_counts"] = user_data_counts
        
        return analysis
    
    def search_user_data_complete(self, user_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì‚¬ìš©ìì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        print(f"\nğŸ¯ ì‚¬ìš©ì '{user_id}' ì „ì²´ ë°ì´í„° ê²€ìƒ‰...")
        
        try:
            dummy_vector = [0.0] * 1024
            
            # í•´ë‹¹ ì‚¬ìš©ìì˜ ëª¨ë“  ë°ì´í„° ê²€ìƒ‰
            query_params = {
                "vector": dummy_vector,
                "filter": {"user_id": str(user_id)},
                "top_k": 10000,  # ì¶©ë¶„íˆ í° ìˆ˜
                "include_metadata": True
            }
            
            if self.namespace:
                query_params["namespace"] = self.namespace
            
            results = self.index.query(**query_params)
            
            user_data = []
            for match in results.matches:
                user_data.append({
                    "id": match.id,
                    "score": match.score,
                    "metadata": match.metadata
                })
            
            print(f"âœ… ì‚¬ìš©ì '{user_id}' ë°ì´í„° {len(user_data)}ê±´ ë°œê²¬")
            
            # ë°ì´í„° ìƒì„¸ ì¶œë ¥
            if user_data:
                print(f"\nğŸ“‹ ì‚¬ìš©ì '{user_id}' ë°ì´í„° ìƒì„¸:")
                for i, item in enumerate(user_data, 1):
                    metadata = item["metadata"]
                    print(f"  {i}. ê¸°ê°„: {metadata.get('start_date')} ~ {metadata.get('end_date')}")
                    print(f"     ì—…ë¬´: {metadata.get('done_task', '')[:100]}...")
                    print()
            
            return user_data
            
        except Exception as e:
            print(f"âŒ ì‚¬ìš©ì ë°ì´í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def export_complete_analysis(self, analysis: Dict[str, Any], filename: str = None) -> str:
        """ì™„ì „í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"complete_pinecone_analysis_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ì™„ì „í•œ ë¶„ì„ ê²°ê³¼ ì €ì¥: {filename}")
            return filename
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” === ì™„ì „í•œ Pinecone ë°ì´í„° ë¶„ì„ê¸° ===")
    print("ëª¨ë“  ë²¡í„° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì™„ì „íˆ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    # API í‚¤ ì„¤ì •
    api_key = "pcsk_5Wcu2A_7QAdTAjfmSYxwxc2sfiZ7G1bhmi9qy6J2KXL1hUfNcoLQ3xGdavA7S4E9DEqpmH"
    index_name = "skore-20250624-144422"
    
    try:
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = CompletePineconeAnalyzer(api_key, index_name)
        
        # ì™„ì „í•œ ë°ì´í„° ë¶„ì„ ì‹¤í–‰
        analysis = analyzer.get_complete_data_analysis()
        
        if not analysis:
            print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì‚¬ìš©ì ì„ íƒ ë©”ë‰´
        while True:
            print(f"\nğŸ¯ === ì¶”ê°€ ì‘ì—… ì„ íƒ ===")
            print("1. íŠ¹ì • ì‚¬ìš©ì ì „ì²´ ë°ì´í„° ê²€ìƒ‰")
            print("2. ë¶„ì„ ê²°ê³¼ JSON ì €ì¥")
            print("3. ì‚¬ìš©ìë³„ ë°ì´í„° í†µê³„ ì¬ì¶œë ¥")
            print("4. ì¢…ë£Œ")
            
            choice = input("\nì„ íƒí•˜ì„¸ìš” (1-4): ").strip()
            
            if choice == "1":
                available_users = analysis.get('unique_user_ids', [])
                print(f"\nì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ìš©ì ID: {available_users}")
                user_id = input("ê²€ìƒ‰í•  ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                if user_id:
                    analyzer.search_user_data_complete(user_id)
            
            elif choice == "2":
                filename = input("ì €ì¥í•  íŒŒì¼ëª… (ì—”í„°ì‹œ ìë™ ìƒì„±): ").strip()
                if not filename:
                    filename = None
                analyzer.export_complete_analysis(analysis, filename)
            
            elif choice == "3":
                print(f"\nğŸ“Š === ì‚¬ìš©ìë³„ ë°ì´í„° í†µê³„ ===")
                user_counts = analysis.get('user_data_counts', {})
                total_users = len(user_counts)
                total_records = sum(user_counts.values())
                
                print(f"ì´ ì‚¬ìš©ì ìˆ˜: {total_users}ëª…")
                print(f"ì´ ë°ì´í„° ìˆ˜: {total_records}ê±´")
                print(f"ì‚¬ìš©ìë‹¹ í‰ê·  ë°ì´í„°: {total_records/total_users:.1f}ê±´")
                print(f"\nì‚¬ìš©ìë³„ ìƒì„¸:")
                for user_id in sorted(user_counts.keys()):
                    print(f"  User {user_id}: {user_counts[user_id]}ê±´")
            
            elif choice == "4":
                print("ğŸ‘‹ ë¶„ì„ê¸°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-4 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()