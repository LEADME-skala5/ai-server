import pandas as pd
import json
import openai
from typing import Dict, List, Any, Optional, Tuple
import os
from datetime import datetime
from pathlib import Path
import logging
import pymysql
from pinecone import Pinecone, ServerlessSpec
import random

# .env íŒŒì¼ ì§€ì› (ì„ íƒì‚¬í•­)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ë¬´ì‹œ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WeeklyReportEvaluationAgent:
    def __init__(self, 
                openai_api_key: Optional[str] = None,
                pinecone_api_key: Optional[str] = None,
                model: Optional[str] = None,
                output_path: Optional[str] = None):
        """
        AI ê¸°ë°˜ ì£¼ê°„ ë³´ê³ ì„œ í‰ê°€ ì—ì´ì „íŠ¸ (Pinecone + MariaDB ë²„ì „)
        
        Args:
            openai_api_key: OpenAI API í‚¤ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
            pinecone_api_key: Pinecone API í‚¤ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
            model: ì‚¬ìš©í•  LLM ëª¨ë¸ëª… (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
            output_path: ê²°ê³¼ íŒŒì¼ë“¤ì„ ì €ì¥í•  ê²½ë¡œ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
        """
        # OpenAI API í‚¤ ì„¤ì •
        final_openai_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not final_openai_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # Pinecone API í‚¤ ì„¤ì •
        final_pinecone_key = pinecone_api_key or os.getenv("PINECONE_API_KEY")
        if not final_pinecone_key:
            raise ValueError("Pinecone API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°)
        self.model = model or os.getenv("OPENAI_MODEL", "gpt-4-turbo")
        
        # ì¶œë ¥ ê²½ë¡œ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°)
        output_path = output_path or os.getenv("OUTPUT_PATH", "./output")
        
        self.openai_client = openai.OpenAI(api_key=final_openai_key)
        self.output_path = Path(output_path)
        
        # Pinecone ì´ˆê¸°í™”
        self.pc = Pinecone(api_key=final_pinecone_key)
        self.pinecone_index_name = os.getenv("PINECONE_INDEX_NAME", "skore-20250624-144422")
        self.index = self.pc.Index(self.pinecone_index_name)
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìë™ ê°ì§€
        self.namespace = self._detect_namespace()
        
        # MariaDB ì—°ê²° ì •ë³´ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        self.db_config = {
            'host': os.getenv("DB_HOST", '13.209.110.151'),
            'port': int(os.getenv("DB_PORT", 27017)),
            'user': os.getenv("DB_USER", 'root'),
            'password': os.getenv("DB_PASSWORD", 'root'),
            'database': os.getenv("DB_DATABASE", 'skala'),
            'charset': os.getenv("DB_CHARSET", 'utf8mb4')
        }
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.weekly_data = None
        self.team_criteria = None
        self.team_goals = None
        
        # ì—ì´ì „íŠ¸ ìƒíƒœ ì¶”ì 
        self.evaluation_history = []
        self.current_context = {}
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_path.mkdir(exist_ok=True)
        
        logger.info(f"WeeklyReportEvaluationAgent ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸: {self.model}")
        logger.info(f"Pinecone ì¸ë±ìŠ¤: {self.pinecone_index_name}")
        logger.info(f"ì‚¬ìš© ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {self.namespace}")
        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤: {self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}")
    
    def _detect_namespace(self) -> str:
        """Pinecone ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ìë™ ê°ì§€í•©ë‹ˆë‹¤."""
        try:
            stats = self.index.describe_index_stats()
            if hasattr(stats, 'namespaces') and stats.namespaces:
                namespaces = list(stats.namespaces.keys())
                if namespaces:
                    # ë°ì´í„°ê°€ ìˆëŠ” ì²« ë²ˆì§¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš©
                    for ns in namespaces:
                        if stats.namespaces[ns].vector_count > 0:
                            logger.info(f"ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê°ì§€: '{ns}' (ë²¡í„° ìˆ˜: {stats.namespaces[ns].vector_count})")
                            return ns
                    return namespaces[0]
            return ""
        except Exception as e:
            logger.warning(f"ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {e}")
            return ""
        
    def connect_to_mariadb(self):
        """MariaDBì— ì—°ê²°í•©ë‹ˆë‹¤."""
        try:
            connection = pymysql.connect(**self.db_config)
            logger.info("MariaDB ì—°ê²° ì„±ê³µ")
            return connection
        except Exception as e:
            logger.error(f"MariaDB ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            raise ValueError(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def load_team_data_from_rdb(self) -> Dict[str, Any]:
        """MariaDBì—ì„œ íŒ€ ê¸°ì¤€ ë° ëª©í‘œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        logger.info("MariaDBì—ì„œ íŒ€ ë°ì´í„° ë¡œë“œ ì‹œì‘")
        
        try:
            connection = self.connect_to_mariadb()
            
            # team_criteria í…Œì´ë¸” ë¡œë“œ
            criteria_query = "SELECT * FROM team_criteria"
            self.team_criteria = pd.read_sql(criteria_query, connection)
            logger.info(f"team_criteria ë¡œë“œ ì™„ë£Œ: {len(self.team_criteria)}ê±´")
            
            # team_goal í…Œì´ë¸” ë¡œë“œ
            goals_query = "SELECT * FROM team_goal"
            self.team_goals = pd.read_sql(goals_query, connection)
            logger.info(f"team_goal ë¡œë“œ ì™„ë£Œ: {len(self.team_goals)}ê±´")
            
            connection.close()
            
            return {
                "team_criteria_records": len(self.team_criteria),
                "team_goals_records": len(self.team_goals),
                "teams_available": self._extract_teams_from_rdb_data()
            }
            
        except Exception as e:
            logger.error(f"íŒ€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise ValueError(f"íŒ€ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def load_weekly_data_from_pinecone(self, user_id: str) -> Dict[str, Any]:
        """Pineconeì—ì„œ íŠ¹ì • ì‚¬ìš©ìì˜ ì£¼ê°„ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        logger.info(f"Pineconeì—ì„œ ì‚¬ìš©ì {user_id} ë°ì´í„° ê²€ìƒ‰ ì‹œì‘")
        
        try:
            # ì‚¬ìš©ì ID ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            # ë”ë¯¸ ë²¡í„°ë¡œ ê²€ìƒ‰ (ì‹¤ì œë¡œëŠ” ë©”íƒ€ë°ì´í„° í•„í„°ë§ë§Œ ì‚¬ìš©)
            dummy_vector = [0.0] * 1024
            
            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì„¤ì •
            query_params = {
                "vector": dummy_vector,
                "filter": {"user_id": str(user_id)},  # ëª…ì‹œì ìœ¼ë¡œ ë¬¸ìì—´ ë³€í™˜
                "top_k": 100,  # ì¶©ë¶„íˆ í° ìˆ˜ë¡œ ì„¤ì •
                "include_metadata": True
            }
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if self.namespace:
                query_params["namespace"] = self.namespace
            
            search_results = self.index.query(**query_params)
            
            logger.info(f"Pinecone ê²€ìƒ‰ ê²°ê³¼: {len(search_results.matches)}ê±´")
            
            if not search_results.matches:
                raise ValueError(f"ì‚¬ìš©ì ID {user_id}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ Pineconeì— ì—†ìŠµë‹ˆë‹¤.")
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ DataFrame í˜•íƒœë¡œ ë³€í™˜
            weekly_records = []
            reference_data = []
            
            for match in search_results.matches:
                metadata = match.metadata
                
                # weekly.csvì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ ë³€í™˜
                record = {
                    'employee_number': metadata.get('user_id'),
                    'name': f"User_{metadata.get('user_id')}",  # ì‹¤ì œ ì´ë¦„ì€ ë³„ë„ í…Œì´ë¸”ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
                    'done_task': metadata.get('done_task', ''),
                    'start_date': metadata.get('start_date'),
                    'end_date': metadata.get('end_date'),
                    'evaluation_year': metadata.get('evaluation_year'),
                    'evaluation_quarter': metadata.get('evaluation_quarter'),
                    'organization_id': metadata.get('organization_id'),
                    'source_file': metadata.get('source_file', ''),
                    'row_index': metadata.get('row_index', '')
                }
                
                weekly_records.append(record)
                
                # reference ì •ë³´ ìˆ˜ì§‘
                reference_data.append({
                    'id': match.id,
                    'score': match.score,
                    'metadata': metadata,
                    'text_preview': metadata.get('text', '')[:200] + '...' if metadata.get('text') else ''
                })
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            self.weekly_data = pd.DataFrame(weekly_records)
            
            # ì¤‘ë³µ ì œê±° (ê°™ì€ ì£¼ì°¨ ë°ì´í„°ê°€ ì¤‘ë³µë  ìˆ˜ ìˆìŒ)
            self.weekly_data = self.weekly_data.drop_duplicates(
                subset=['employee_number', 'start_date', 'end_date'], 
                keep='first'
            )
            
            logger.info(f"ì£¼ê°„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.weekly_data)}ê±´ (ì¤‘ë³µ ì œê±° í›„)")
            
            return {
                "weekly_records": len(self.weekly_data),
                "pinecone_matches": len(search_results.matches),
                "date_range": self._extract_date_range(),
                "reference_data": reference_data
            }
            
        except Exception as e:
            logger.error(f"Pinecone ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise ValueError(f"Pinecone ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def plan_evaluation(self, 
                       target_user_id: str,
                       target_employees: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        í‰ê°€ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
        """
        logger.info("=== í‰ê°€ ê³„íš ìˆ˜ë¦½ ì‹œì‘ ===")
        
        plan = {
            "timestamp": datetime.now().isoformat(),
            "data_sources": {
                "weekly_data": "Pinecone Vector DB",
                "team_criteria": "MariaDB - team_criteria í…Œì´ë¸”",
                "team_goals": "MariaDB - team_goal í…Œì´ë¸”"
            },
            "target_user_id": target_user_id,
            "target_employees": target_employees,
            "steps": []
        }
        
        # 1ë‹¨ê³„: ë°ì´í„° ì†ŒìŠ¤ ì—°ê²° ê²€ì¦
        plan["steps"].append({
            "step": 1,
            "action": "Pinecone ë° MariaDB ì—°ê²° ê²€ì¦",
            "status": "planned"
        })
        
        # 2ë‹¨ê³„: ë°ì´í„° ë¡œë“œ
        plan["steps"].append({
            "step": 2,
            "action": "Pineconeì—ì„œ ì£¼ê°„ ë°ì´í„°, MariaDBì—ì„œ íŒ€ ë°ì´í„° ë¡œë“œ",
            "status": "planned"
        })
        
        # 3ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ë° ê²€ì¦
        plan["steps"].append({
            "step": 3,
            "action": "ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ë° ì „ì²˜ë¦¬",
            "status": "planned"
        })
        
        # 4ë‹¨ê³„: í‰ê°€ ì‹¤í–‰
        plan["steps"].append({
            "step": 4,
            "action": "AI ê¸°ë°˜ ê°œë³„ ì§ì› í‰ê°€ ìˆ˜í–‰",
            "status": "planned"
        })
        
        # 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥ (reference í¬í•¨)
        plan["steps"].append({
            "step": 5,
            "action": "í‰ê°€ ê²°ê³¼ ë° ì°¸ì¡° ì •ë³´ ì €ì¥",
            "status": "planned"
        })
        
        self.current_context["plan"] = plan
        logger.info(f"í‰ê°€ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ - {len(plan['steps'])}ë‹¨ê³„")
        
        return plan
    
    def validate_data_sources(self, user_id: str) -> Dict[str, Any]:
        """
        ë°ì´í„° ì†ŒìŠ¤ë“¤ì˜ ì—°ê²° ìƒíƒœì™€ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        logger.info("ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì‹œì‘")
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "source_info": {}
        }
        
        # Pinecone ì—°ê²° ë° ë°ì´í„° í™•ì¸
        try:
            # ì¸ë±ìŠ¤ ì •ë³´ í™•ì¸
            index_stats = self.index.describe_index_stats()
            validation_result["source_info"]["pinecone"] = {
                "index_name": self.pinecone_index_name,
                "total_vectors": index_stats.total_vector_count,
                "dimension": 1024,
                "namespace": self.namespace,
                "status": "connected"
            }
            
            # íŠ¹ì • ì‚¬ìš©ì ë°ì´í„° ì¡´ì¬ í™•ì¸
            dummy_vector = [0.0] * 1024
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì„¤ì •
            query_params = {
                "vector": dummy_vector,
                "filter": {"user_id": str(user_id)},  # ë¬¸ìì—´ë¡œ ë³€í™˜
                "top_k": 1,
                "include_metadata": True
            }
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if self.namespace:
                query_params["namespace"] = self.namespace
            
            test_search = self.index.query(**query_params)
            
            if not test_search.matches:
                validation_result["errors"].append(f"ì‚¬ìš©ì ID {user_id}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ Pineconeì— ì—†ìŠµë‹ˆë‹¤.")
                validation_result["valid"] = False
            else:
                validation_result["source_info"]["pinecone"]["user_data_found"] = True
                
        except Exception as e:
            validation_result["errors"].append(f"Pinecone ì—°ê²° ì˜¤ë¥˜: {str(e)}")
            validation_result["valid"] = False
        
        # MariaDB ì—°ê²° ë° í…Œì´ë¸” í™•ì¸
        try:
            connection = self.connect_to_mariadb()
            cursor = connection.cursor()
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor.execute("SHOW TABLES LIKE 'team_criteria'")
            if not cursor.fetchone():
                validation_result["errors"].append("team_criteria í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                validation_result["valid"] = False
            
            cursor.execute("SHOW TABLES LIKE 'team_goal'")
            if not cursor.fetchone():
                validation_result["errors"].append("team_goal í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                validation_result["valid"] = False
            
            # í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜ í™•ì¸
            cursor.execute("SELECT COUNT(*) FROM team_criteria")
            criteria_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM team_goal")
            goals_count = cursor.fetchone()[0]
            
            validation_result["source_info"]["mariadb"] = {
                "host": self.db_config["host"],
                "database": self.db_config["database"],
                "team_criteria_records": criteria_count,
                "team_goals_records": goals_count,
                "status": "connected"
            }
            
            connection.close()
            
        except Exception as e:
            validation_result["errors"].append(f"MariaDB ì—°ê²° ì˜¤ë¥˜: {str(e)}")
            validation_result["valid"] = False
        
        if validation_result["valid"]:
            logger.info("ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì„±ê³µ")
        else:
            logger.error(f"ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {validation_result['errors']}")
            
        return validation_result.info("ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì‹œì‘")
        
    def validate_data_sources(self, user_id: str) -> Dict[str, Any]:
        """
        ë°ì´í„° ì†ŒìŠ¤ë“¤ì˜ ì—°ê²° ìƒíƒœì™€ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        logger.info("ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì‹œì‘")
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "source_info": {}
        }
        
        # Pinecone ì—°ê²° ë° ë°ì´í„° í™•ì¸
        try:
            # ì¸ë±ìŠ¤ ì •ë³´ í™•ì¸
            index_stats = self.index.describe_index_stats()
            validation_result["source_info"]["pinecone"] = {
                "index_name": self.pinecone_index_name,
                "total_vectors": index_stats.total_vector_count,
                "dimension": 1024,
                "namespace": self.namespace,
                "status": "connected"
            }
            
            # íŠ¹ì • ì‚¬ìš©ì ë°ì´í„° ì¡´ì¬ í™•ì¸
            dummy_vector = [0.0] * 1024
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì„¤ì •
            query_params = {
                "vector": dummy_vector,
                "filter": {"user_id": str(user_id)},  # ë¬¸ìì—´ë¡œ ë³€í™˜
                "top_k": 1,
                "include_metadata": True
            }
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if self.namespace:
                query_params["namespace"] = self.namespace
            
            test_search = self.index.query(**query_params)
            
            if not test_search.matches:
                validation_result["errors"].append(f"ì‚¬ìš©ì ID {user_id}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ Pineconeì— ì—†ìŠµë‹ˆë‹¤.")
                validation_result["valid"] = False
            else:
                validation_result["source_info"]["pinecone"]["user_data_found"] = True
                
        except Exception as e:
            validation_result["errors"].append(f"Pinecone ì—°ê²° ì˜¤ë¥˜: {str(e)}")
            validation_result["valid"] = False
        
        # MariaDB ì—°ê²° ë° í…Œì´ë¸” í™•ì¸
        try:
            connection = self.connect_to_mariadb()
            cursor = connection.cursor()
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor.execute("SHOW TABLES LIKE 'team_criteria'")
            if not cursor.fetchone():
                validation_result["errors"].append("team_criteria í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                validation_result["valid"] = False
            
            cursor.execute("SHOW TABLES LIKE 'team_goal'")
            if not cursor.fetchone():
                validation_result["errors"].append("team_goal í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                validation_result["valid"] = False
            
            # í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜ í™•ì¸
            cursor.execute("SELECT COUNT(*) FROM team_criteria")
            criteria_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM team_goal")
            goals_count = cursor.fetchone()[0]
            
            validation_result["source_info"]["mariadb"] = {
                "host": self.db_config["host"],
                "database": self.db_config["database"],
                "team_criteria_records": criteria_count,
                "team_goals_records": goals_count,
                "status": "connected"
            }
            
            connection.close()
            
        except Exception as e:
            validation_result["errors"].append(f"MariaDB ì—°ê²° ì˜¤ë¥˜: {str(e)}")
            validation_result["valid"] = False
        
        if validation_result["valid"]:
            logger.info("ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì„±ê³µ")
        else:
            logger.error(f"ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {validation_result['errors']}")
            
        return validation_result
    
    def load_and_preprocess_data(self, user_id: str) -> Dict[str, Any]:
        """
        Pineconeê³¼ MariaDBì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        logger.info("ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘")
        
        try:
            # 1. MariaDBì—ì„œ íŒ€ ë°ì´í„° ë¡œë“œ
            team_data_result = self.load_team_data_from_rdb()
            
            # 2. Pineconeì—ì„œ ì£¼ê°„ ë°ì´í„° ë¡œë“œ
            weekly_data_result = self.load_weekly_data_from_pinecone(user_id)
            
            # ì „ì²˜ë¦¬ ê²°ê³¼ í†µí•©
            preprocessing_result = {
                "weekly_records": weekly_data_result["weekly_records"],
                "pinecone_matches": weekly_data_result["pinecone_matches"],
                "team_criteria_records": team_data_result["team_criteria_records"],
                "team_goals_records": team_data_result["team_goals_records"],
                "unique_employees": self.weekly_data['employee_number'].nunique() if self.weekly_data is not None else 0,
                "date_range": weekly_data_result["date_range"],
                "reference_data": weekly_data_result["reference_data"],
                "teams_available": team_data_result["teams_available"]
            }
            
            logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ - ì£¼ê°„ ê¸°ë¡: {preprocessing_result['weekly_records']}ê±´, "
                       f"íŒ€ ê¸°ì¤€: {preprocessing_result['team_criteria_records']}ê±´, "
                       f"íŒ€ ëª©í‘œ: {preprocessing_result['team_goals_records']}ê±´")
            return preprocessing_result
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise ValueError(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def _extract_teams_from_rdb_data(self) -> List[str]:
        """RDB ë°ì´í„°ì—ì„œ íŒ€ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        teams = set()
        
        # team_goals ë°ì´í„°ì—ì„œ íŒ€ ì¶”ì¶œ
        if self.team_goals is not None:
            team_column = self._find_column_by_keywords(
                self.team_goals, 
                ['team', 'org', 'group', 'dept', 'íŒ€', 'ì¡°ì§', 'ë¶€ì„œ', 'organization']
            )
            if team_column:
                teams.update(self.team_goals[team_column].dropna().unique())
        
        # team_criteria ë°ì´í„°ì—ì„œ íŒ€ ì¶”ì¶œ
        if self.team_criteria is not None:
            team_column = self._find_column_by_keywords(
                self.team_criteria, 
                ['team', 'org', 'group', 'dept', 'íŒ€', 'ì¡°ì§', 'ë¶€ì„œ', 'organization']
            )
            if team_column:
                teams.update(self.team_criteria[team_column].dropna().unique())
        
        return sorted(list(teams))
    
    def _extract_date_range(self) -> Dict[str, str]:
        """ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.weekly_data is None:
            return {}
            
        date_info = {}
        for date_col in ['start_date', 'end_date', 'date']:
            if date_col in self.weekly_data.columns:
                dates = pd.to_datetime(self.weekly_data[date_col], errors='coerce').dropna()
                if not dates.empty:
                    date_info[f"{date_col}_min"] = dates.min().strftime('%Y-%m-%d')
                    date_info[f"{date_col}_max"] = dates.max().strftime('%Y-%m-%d')
        
        return date_info
    
    def analyze_employee_data(self, employee_number: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ì§ì›ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        """
        logger.info(f"ì§ì› {employee_number} ë°ì´í„° ë¶„ì„ ì‹œì‘")
        
        if self.weekly_data is None:
            raise ValueError("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        employee_data = self.weekly_data[
            self.weekly_data['employee_number'] == employee_number
        ].copy()
        
        if employee_data.empty:
            raise ValueError(f"ì§ì›ë²ˆí˜¸ {employee_number}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì§ì› ì •ë³´ ì¶”ì¶œ
        context = {
            "employee_info": self._extract_employee_info(employee_data),
            "team_goals": self._get_filtered_team_goals(employee_data),
            "team_criteria": self._get_filtered_team_criteria(employee_data),
            "weekly_tasks": employee_data[['start_date', 'end_date', 'done_task']].to_dict('records'),
            "reference_info": self._get_reference_info(employee_number)
        }
        
        logger.info(f"ì§ì› {employee_number} ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
        return context
    
    def _get_reference_info(self, employee_number: str) -> Dict[str, Any]:
        """ì°¸ì¡° ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # current_contextì—ì„œ reference_data ê°€ì ¸ì˜¤ê¸° (load_and_preprocess_dataì—ì„œ ì €ì¥ë¨)
        reference_data = self.current_context.get("preprocessing_result", {}).get("reference_data", [])
        
        return {
            "source_type": "Pinecone Vector Database",
            "index_name": self.pinecone_index_name,
            "search_method": "metadata_filter",
            "filter_criteria": {"user_id": str(employee_number)},  # ë¬¸ìì—´ë¡œ ë³€í™˜
            "documents_found": len(reference_data),
            "pinecone_matches": reference_data[:10],  # ìƒìœ„ 10ê°œë§Œ ì €ì¥
            "data_sources": {
                "weekly_data": "Pinecone Vector DB",
                "team_criteria": "MariaDB.team_criteria",
                "team_goals": "MariaDB.team_goal"
            }
        }
    
    def _extract_employee_info(self, employee_data: pd.DataFrame) -> Dict[str, Any]:
        """ì§ì› ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        info = {
            "name": employee_data['name'].iloc[0] if 'name' in employee_data.columns else f"User_{employee_data['employee_number'].iloc[0]}",
            "employee_number": employee_data['employee_number'].iloc[0],
            "organization_id": employee_data['organization_id'].iloc[0] if 'organization_id' in employee_data.columns else "",
            "evaluation_year": employee_data['evaluation_year'].iloc[0] if 'evaluation_year' in employee_data.columns else "",
            "evaluation_quarter": employee_data['evaluation_quarter'].iloc[0] if 'evaluation_quarter' in employee_data.columns else "",
            "period": "",
            "total_weeks": len(employee_data),
            "total_activities": len(employee_data)
        }
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        if 'start_date' in employee_data.columns and 'end_date' in employee_data.columns:
            start_dates = pd.to_datetime(employee_data['start_date'], errors='coerce').dropna()
            end_dates = pd.to_datetime(employee_data['end_date'], errors='coerce').dropna()
            if not start_dates.empty and not end_dates.empty:
                info["period"] = f"{start_dates.min().strftime('%Y-%m-%d')} ~ {end_dates.max().strftime('%Y-%m-%d')}"
        
        logger.info(f"ì§ì› ì •ë³´: {info['name']} (ì¡°ì§ID: {info['organization_id']})")
        return info
    
    def _get_filtered_team_goals(self, employee_data: pd.DataFrame) -> List[Dict]:
        """í•´ë‹¹ ì§ì›ì˜ íŒ€ ëª©í‘œë§Œ í•„í„°ë§í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if self.team_goals is None or self.team_goals.empty:
            return []
            
        # organization_idë¡œ í•„í„°ë§
        employee_org_id = employee_data['organization_id'].iloc[0] if 'organization_id' in employee_data.columns else ""
        
        if employee_org_id:
            # organization_id ì»¬ëŸ¼ ì°¾ê¸°
            org_column = self._find_column_by_keywords(
                self.team_goals, 
                ['organization_id', 'org_id', 'team_id', 'ì¡°ì§', 'íŒ€']
            )
            
            if org_column:
                filtered_goals = self.team_goals[
                    self.team_goals[org_column].astype(str) == str(employee_org_id)
                ].to_dict('records')
                logger.info(f"íŒ€ ëª©í‘œ í•„í„°ë§ ì™„ë£Œ: {len(filtered_goals)}ê°œ ëª©í‘œ")
                return filtered_goals
        
        logger.warning("ì¡°ì§ ID ë§¤ì¹­ ì‹¤íŒ¨, ì „ì²´ ëª©í‘œ ë°˜í™˜")
        return self.team_goals.to_dict('records')
    
    def _get_filtered_team_criteria(self, employee_data: pd.DataFrame) -> List[Dict]:
        """í•´ë‹¹ ì§ì›ì˜ íŒ€ í‰ê°€ ê¸°ì¤€ë§Œ í•„í„°ë§í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if self.team_criteria is None or self.team_criteria.empty:
            return []
            
        # organization_idë¡œ í•„í„°ë§
        employee_org_id = employee_data['organization_id'].iloc[0] if 'organization_id' in employee_data.columns else ""
        
        if employee_org_id:
            # organization_id ì»¬ëŸ¼ ì°¾ê¸°
            org_column = self._find_column_by_keywords(
                self.team_criteria, 
                ['organization_id', 'org_id', 'team_id', 'ì¡°ì§', 'íŒ€']
            )
            
            if org_column:
                filtered_criteria = self.team_criteria[
                    self.team_criteria[org_column].astype(str) == str(employee_org_id)
                ].to_dict('records')
                logger.info(f"íŒ€ ê¸°ì¤€ í•„í„°ë§ ì™„ë£Œ: {len(filtered_criteria)}ê°œ ê¸°ì¤€")
                return filtered_criteria
        
        logger.warning("ì¡°ì§ ID ë§¤ì¹­ ì‹¤íŒ¨, ì „ì²´ ê¸°ì¤€ ë°˜í™˜")
        return self.team_criteria.to_dict('records')
    
    def _convert_date_to_week_format(self, start_date: str, end_date: str) -> str:
        """ë‚ ì§œë¥¼ 'Nì›” Nì£¼ì°¨' í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            from datetime import datetime
            import calendar
            
            # ì‹œì‘ ë‚ ì§œ íŒŒì‹±
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            
            # ì›”ê³¼ í•´ë‹¹ ì›”ì˜ ëª‡ ë²ˆì§¸ ì£¼ì¸ì§€ ê³„ì‚°
            month = start_dt.month
            
            # í•´ë‹¹ ì›”ì˜ ì²« ë²ˆì§¸ ë‚ 
            first_day = datetime(start_dt.year, month, 1)
            
            # ì²« ë²ˆì§¸ ë‚ ì˜ ìš”ì¼ (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
            first_weekday = first_day.weekday()
            
            # ì‹œì‘ ë‚ ì§œê°€ ëª‡ ë²ˆì§¸ ì£¼ì¸ì§€ ê³„ì‚°
            # ì›”ì˜ ì²« ë²ˆì§¸ ì›”ìš”ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì£¼ì°¨ ê³„ì‚°
            days_from_first = (start_dt - first_day).days
            week_number = (days_from_first + first_weekday) // 7 + 1
            
            return f"{month}ì›” {week_number}ì£¼ì°¨ weekly ë³´ê³ ì„œ"
        
        except Exception as e:
            logger.warning(f"ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ ({start_date} ~ {end_date}): {e}")
            return f"weekly ë³´ê³ ì„œ ({start_date}~{end_date})"
    
    def _find_column_by_keywords(self, 
                                dataframe: pd.DataFrame, 
                                keywords: List[str]) -> Optional[str]:
        """í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»¬ëŸ¼ì„ ì°¾ìŠµë‹ˆë‹¤."""
        for col in dataframe.columns:
            col_lower = str(col).lower().strip()
            if any(keyword.lower() in col_lower for keyword in keywords):
                return col
        return None
    
    def extract_team_goal_categories(self, team_goals: List[Dict]) -> List[str]:
        """íŒ€ ëª©í‘œì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not team_goals:
            logger.warning("íŒ€ ëª©í‘œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ëª©í‘œ ê´€ë ¨ í‚¤ ì°¾ê¸°
        goal_keywords = ['goal_name', 'task_name', 'objective', 'goal', 'task', 'ì„±ê³¼ì§€í‘œëª…', 'ê³¼ì œëª…', 'ëª©í‘œëª…']
        exclude_keywords = ['name', 'ì´ë¦„', 'ì„±ëª…']
        
        first_record = team_goals[0]
        goal_key = None
        
        # ì •í™•í•œ ë§¤ì¹­ ìš°ì„ 
        for key in goal_keywords:
            if key in first_record:
                goal_key = key
                break
        
        # í‚¤ì›Œë“œ í¬í•¨ ê²€ìƒ‰
        if not goal_key:
            for key in first_record.keys():
                key_lower = str(key).lower()
                if any(keyword in key_lower for keyword in ['goal', 'task', 'ê³¼ì œ', 'ëª©í‘œ', 'ì§€í‘œ']):
                    if not any(exclude in key_lower for exclude in exclude_keywords) or 'goal' in key_lower:
                        goal_key = key
                        break
        
        if goal_key:
            categories = list(set([
                str(record[goal_key]).strip() for record in team_goals 
                if record.get(goal_key) and str(record[goal_key]).strip() and str(record[goal_key]).strip().lower() != 'nan'
            ]))
            logger.info(f"ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì™„ë£Œ: {categories}")
            return categories
        else:
            logger.error("ëª©í‘œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
    
    def generate_evaluation_prompt(self, employee_data: Dict[str, Any]) -> str:
        """í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        team_categories = self.extract_team_goal_categories(employee_data['team_goals'])
        
        if not team_categories:
            # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©
            team_categories = [
                "oud Professional ì—…ë¬´ ì§„í–‰ í†µí•œ BR/UR ê°œì„ ",
                "CSP íŒŒíŠ¸ë„ˆì‰½ ê°•í™” í†µí•œ ì›ê°€ê°œì„ ", 
                "oud ë§ˆì¼€íŒ… ë° í™ë³´ í†µí•œ ëŒ€ì™¸ oud ê³ ê°í™•ë³´",
                "ê¸€ë¡œë²Œ ì‚¬ì—… Tech-presales ì§„í–‰"
            ]
            logger.warning(f"íŒ€ ëª©í‘œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©: {team_categories}")
        
        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ HR í‰ê°€ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ì§ì›ì˜ ì£¼ê°„ ë³´ê³ ì„œë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ ê°ê´€ì ì¸ ì„±ê³¼ í‰ê°€ë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

## í‰ê°€ ëŒ€ìƒ ì •ë³´

### ì§ì› ê¸°ë³¸ ì •ë³´
- ì´ë¦„: {employee_data['employee_info']['name']}
- ì§ì›ë²ˆí˜¸(User ID): {employee_data['employee_info']['employee_number']}
- ì¡°ì§ ID: {employee_data['employee_info']['organization_id']}
- í‰ê°€ ê¸°ê°„: {employee_data['employee_info']['period']}
- í‰ê°€ ë…„ë„/ë¶„ê¸°: {employee_data['employee_info']['evaluation_year']}ë…„ {employee_data['employee_info']['evaluation_quarter']}ë¶„ê¸°
- ì´ í‰ê°€ ì£¼ì°¨: {employee_data['employee_info']['total_weeks']}ì£¼

### ì£¼ê°„ë³„ ìˆ˜í–‰ ì—…ë¬´
"""
        
        # ì£¼ê°„ë³„ ì—…ë¬´ ì¶”ê°€
        if employee_data['weekly_tasks']:
            for i, task in enumerate(employee_data['weekly_tasks'], 1):
                start_date = task.get('start_date', 'N/A')
                end_date = task.get('end_date', 'N/A')
                done_task = task.get('done_task', 'N/A')
                prompt += f"\n**{i}ì£¼ì°¨ ({start_date} ~ {end_date})**\n"
                prompt += f"{done_task}\n"
        else:
            prompt += "\nì£¼ê°„ ì—…ë¬´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
        
        # íŒ€ ëª©í‘œ ì¶”ê°€
        if employee_data['team_goals']:
            prompt += "\n### íŒ€ ëª©í‘œ ë° ì„±ê³¼ì§€í‘œ\n"
            for i, goal in enumerate(employee_data['team_goals'], 1):
                prompt += f"**ëª©í‘œ {i}**: {goal}\n"
        
        # íŒ€ í‰ê°€ ê¸°ì¤€ ì¶”ê°€
        if employee_data['team_criteria']:
            prompt += "\n### íŒ€ í‰ê°€ ê¸°ì¤€\n"
            for i, criteria in enumerate(employee_data['team_criteria'], 1):
                prompt += f"**ê¸°ì¤€ {i}**: {criteria}\n"
        
        prompt += f"""

## í‰ê°€ ê²°ê³¼ í˜•ì‹

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¢…í•© í‰ê°€ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

```json
{{
  "teamGoals": [
    {{
      "goalName": "ëª©í‘œëª…",
      "assigned": "ë°°ì •|ë¯¸ë°°ì •",
      "contributionCount": ê¸°ì—¬í™œë™ìˆ˜,
      "contents": [
        {{
          "description": "êµ¬ì²´ì ì¸ ì—…ë¬´ í™œë™ ì„¤ëª…",
          "reference": [
            {{
              "label": "Nì›” Nì£¼ì°¨ weekly ë³´ê³ ì„œ",
              "excerpt": "í•´ë‹¹ ì£¼ì°¨ ë³´ê³ ì„œì—ì„œ ê´€ë ¨ ì—…ë¬´ ë‚´ìš© ë°œì·Œ"
            }}
          ]
        }}
      ]
    }}
  ]
}}
```

## í‰ê°€ ê°€ì´ë“œë¼ì¸

1. **íŒ€ ëª©í‘œ ë¶„ë¥˜**: ì£¼ê°„ ë³´ê³ ì„œì˜ ì‹¤ì œ ì—…ë¬´ë¥¼ ì•„ë˜ ëª©í‘œë“¤ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
"""
        
        for i, category in enumerate(team_categories, 1):
            prompt += f"   {i}. {category}\n"
        
        prompt += f"""

2. **ë°°ì • ìƒíƒœ íŒë‹¨**: 
   - "ë°°ì •": í•´ë‹¹ ëª©í‘œì™€ ê´€ë ¨ëœ êµ¬ì²´ì  í™œë™ì´ 1ê°œ ì´ìƒ ìˆëŠ” ê²½ìš°
   - "ë¯¸ë°°ì •": í•´ë‹¹ ëª©í‘œì™€ ê´€ë ¨ëœ í™œë™ì´ ì—†ê±°ë‚˜ ë§¤ìš° ê°„ì ‘ì ì¸ ê²½ìš°

3. **ê¸°ì—¬ í™œë™ ì§‘ê³„**:
   - contributionCount: contents ë°°ì—´ì˜ ì‹¤ì œ ê¸¸ì´ì™€ ì¼ì¹˜í•´ì•¼ í•¨
   - ì‹¤ì œ ìˆ˜í–‰í•œ êµ¬ì²´ì  ì—…ë¬´ë§Œ í¬í•¨

4. **ì—…ë¬´ ë‚´ìš© ì¶”ì¶œ**:
   - description: ì‹¤ì œ ìˆ˜í–‰í•œ êµ¬ì²´ì ì¸ ì—…ë¬´ í™œë™
   - excerpt: í•´ë‹¹ ì£¼ì°¨ ë³´ê³ ì„œì—ì„œ ê·¸ í™œë™ì„ ì–¸ê¸‰í•œ ì›ë¬¸ ë°œì·Œ

5. **ì°¸ì¡° ì •ë³´**:
   - label: "Nì›” Nì£¼ì°¨ weekly ë³´ê³ ì„œ" í˜•ì‹
   - excerpt: ì‹¤ì œ ì£¼ê°„ ë³´ê³ ì„œì—ì„œ í•´ë‹¹ í™œë™ì„ ì–¸ê¸‰í•œ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ë°œì·Œ

## ì¤‘ìš”ì‚¬í•­
- ëª¨ë“  íŒ€ ëª©í‘œë¥¼ ë°°ì—´ì— í¬í•¨í•´ì•¼ í•¨ (í™œë™ì´ ì—†ì–´ë„ "ë¯¸ë°°ì •"ìœ¼ë¡œ í¬í•¨)
- excerptëŠ” ì‹¤ì œ ì£¼ê°„ ë³´ê³ ì„œì˜ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë°œì·Œ
- descriptionì€ ì—¬ëŸ¬ ì£¼ì°¨ì˜ ê´€ë ¨ í™œë™ì„ ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ êµ¬ì²´ì  ì—…ë¬´ë¡œ ì •ë¦¬
- ë‚ ì§œ ë³€í™˜: 2024-01-01~2024-01-07 â†’ "1ì›” 1ì£¼ì°¨ weekly ë³´ê³ ì„œ"

JSON í˜•ì‹ì„ ì •í™•íˆ ì¤€ìˆ˜í•˜ì—¬ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
        
        return prompt
    
    def execute_llm_evaluation(self, prompt: str) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            logger.info(f"LLM í‰ê°€ ì‹¤í–‰ - ëª¨ë¸: {self.model}")
            print(f"ğŸ¤– OpenAI API í˜¸ì¶œ ì‹œì‘... (ëª¨ë¸: {self.model})")
            
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system", 
                        "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ HR í‰ê°€ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  êµ¬ì²´ì ì¸ ì„±ê³¼ í‰ê°€ë¥¼ ì œê³µí•˜ë©°, í•­ìƒ ì •í™•í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=4000
            )
            
            response_text = response.choices[0].message.content
            print(f"âœ… OpenAI API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ê¸¸ì´: {len(response_text)} ë¬¸ì)")
            
            # JSON ì¶”ì¶œ ë° íŒŒì‹±
            json_text = self._extract_json_from_response(response_text)
            result = json.loads(json_text)
            
            logger.info("LLM í‰ê°€ ì™„ë£Œ")
            print("âœ… JSON íŒŒì‹± ì„±ê³µ")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {
                "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                "raw_response": response_text if 'response_text' in locals() else "No response",
                "error_details": str(e)
            }
        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            print(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _extract_json_from_response(self, response_text: str) -> str:
        """ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            return response_text[json_start:json_end].strip()
        else:
            return response_text.strip()
    
    def save_evaluation_results(self, 
                               results: Dict[str, Any], 
                               reference_info: Dict[str, Any],
                               filename: Optional[str] = None) -> str:
        """í‰ê°€ ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ teamGoals JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"evaluation_result_{timestamp}.json"
        
        # ìƒˆë¡œìš´ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if "error" not in results and "teamGoals" in results:
            # ì´ë¯¸ ìƒˆë¡œìš´ í˜•ì‹ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            final_results = results
        elif "error" not in results:
            # ê¸°ì¡´ í˜•ì‹ì—ì„œ ìƒˆë¡œìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í˜¸í™˜ì„±ì„ ìœ„í•´)
            team_categories = [
                "Cloud Professional ì—…ë¬´ ì§„í–‰ í†µí•œ BR/UR ê°œì„ ",
                "CSP íŒŒíŠ¸ë„ˆì‰½ ê°•í™” í†µí•œ ì›ê°€ê°œì„ ", 
                "Cloud ë§ˆì¼€íŒ… ë° í™ë³´ í†µí•œ ëŒ€ì™¸ Cloud ê³ ê°í™•ë³´",
                "ê¸€ë¡œë²Œ ì‚¬ì—… Tech-presales ì§„í–‰"
            ]
            
            final_results = {
                "teamGoals": []
            }
            
            # ê° íŒ€ ëª©í‘œì— ëŒ€í•´ ê¸°ë³¸ êµ¬ì¡° ìƒì„±
            for goal_name in team_categories:
                goal_data = {
                    "goalName": goal_name,
                    "assigned": "ë¯¸ë°°ì •",
                    "contributionCount": 0,
                    "contents": []
                }
                final_results["teamGoals"].append(goal_data)
                
        else:
            # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš° ê¸°ì¡´ í˜•ì‹ ìœ ì§€
            final_results = results
        
        # reference ì •ë³´ ì¶”ê°€
        final_results["reference"] = {
            "evaluation_basis": "ì´ í‰ê°€ëŠ” Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ ì£¼ê°„ ë³´ê³ ì„œ ë°ì´í„°ì™€ MariaDBì˜ íŒ€ ëª©í‘œ/ê¸°ì¤€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ AIê°€ ë¶„ì„í•˜ì—¬ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data_sources": reference_info.get("data_sources", {}),
            "pinecone_info": {
                "index_name": reference_info.get("index_name", ""),
                "search_method": reference_info.get("search_method", ""),
                "filter_criteria": reference_info.get("filter_criteria", {}),
                "documents_found": reference_info.get("documents_found", 0),
                "namespace": getattr(self, 'namespace', '')
            },
            "evaluation_timestamp": datetime.now().isoformat(),
            "system_info": {
                "ai_model": self.model,
                "pinecone_index": self.pinecone_index_name,
                "database": f"{self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}"
            }
        }
        
        output_file = self.output_path / filename
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        return str(output_file)
    
    def _is_activity_related(self, activity: str, done_task: str) -> bool:
        """í™œë™ê³¼ ì—…ë¬´ ë‚´ìš©ì˜ ì—°ê´€ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤."""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
            activity_keywords = activity.lower().split()
            done_task_lower = done_task.lower()
            
            # ì£¼ìš” í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            match_count = sum(1 for keyword in activity_keywords if keyword in done_task_lower and len(keyword) > 2)
            
            # ì „ì²´ í‚¤ì›Œë“œì˜ 30% ì´ìƒ ë§¤ì¹­ë˜ë©´ ê´€ë ¨ì„± ìˆë‹¤ê³  íŒë‹¨
            return match_count >= max(1, len(activity_keywords) * 0.3)
        except:
            return False
    
    def execute_single_evaluation(self, 
                                 user_id: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‚¬ìš©ìì— ëŒ€í•œ ì™„ì „í•œ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        logger.info(f"=== ì‚¬ìš©ì {user_id} í‰ê°€ ì‹œì‘ ===")
        
        try:
            # 1ë‹¨ê³„: ê³„íš ìˆ˜ë¦½
            plan = self.plan_evaluation(user_id)
            
            # 2ë‹¨ê³„: ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦
            validation = self.validate_data_sources(user_id)
            if not validation["valid"]:
                raise ValueError(f"ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {validation['errors']}")
            
            # 3ë‹¨ê³„: ë°ì´í„° ë¡œë“œ
            preprocessing_result = self.load_and_preprocess_data(user_id)
            
            # ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥ (reference ì •ë³´ë¥¼ ìœ„í•´)
            self.current_context["preprocessing_result"] = preprocessing_result
            
            # 4ë‹¨ê³„: ì§ì› ë°ì´í„° ë¶„ì„
            employee_data = self.analyze_employee_data(user_id)
            
            # 5ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.generate_evaluation_prompt(employee_data)
            
            # 6ë‹¨ê³„: LLM í‰ê°€ ì‹¤í–‰
            evaluation_result = self.execute_llm_evaluation(prompt)
            
            # 7ë‹¨ê³„: ê²°ê³¼ ì €ì¥ (reference ì •ë³´ í¬í•¨)
            if "error" not in evaluation_result:
                output_file = self.save_evaluation_results(
                    evaluation_result,
                    employee_data["reference_info"],  # reference ì •ë³´ ì „ë‹¬
                    f"evaluation_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                )
                
                # í‰ê°€ ì´ë ¥ì— ì¶”ê°€
                self.evaluation_history.append({
                    "user_id": user_id,
                    "timestamp": datetime.now().isoformat(),
                    "output_file": output_file,
                    "status": "success"
                })
                
                logger.info(f"ì‚¬ìš©ì {user_id} í‰ê°€ ì™„ë£Œ - ì„±ê³µ")
                return evaluation_result
            else:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê°„ë‹¨í•œ ê²°ê³¼ ì €ì¥
                error_result = {
                    "user_id": user_id,
                    "timestamp": datetime.now().isoformat(),
                    "error": evaluation_result["error"]
                }
                
                output_file = self.save_evaluation_results(
                    error_result,
                    {"error": "í‰ê°€ ì‹¤íŒ¨ë¡œ ì¸í•œ ì œí•œëœ ì°¸ì¡° ì •ë³´"},
                    f"evaluation_error_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                )
                
                self.evaluation_history.append({
                    "user_id": user_id,
                    "timestamp": datetime.now().isoformat(),
                    "output_file": output_file,
                    "status": "failed"
                })
                
                logger.info(f"ì‚¬ìš©ì {user_id} í‰ê°€ ì‹¤íŒ¨")
                return error_result
                
        except Exception as e:
            logger.error(f"ì‚¬ìš©ì {user_id} í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            error_result = {
                "user_id": user_id,
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
            return error_result

    def execute_batch_evaluation(self, 
                                target_user_ids: Optional[List[str]] = None) -> Dict[str, Any]:
        """ë‹¤ìˆ˜ ì‚¬ìš©ìì— ëŒ€í•œ ë°°ì¹˜ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        logger.info("=== ë°°ì¹˜ í‰ê°€ ì‹œì‘ ===")
        
        try:
            # ëŒ€ìƒ ì‚¬ìš©ì ëª©ë¡ ê²°ì • (Pineconeì—ì„œ ëª¨ë“  user_id ì¡°íšŒ)
            if target_user_ids is None:
                # get_available_user_ids ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê¹Œì§€ ê³ ë ¤í•œ ì¡°íšŒ
                target_user_ids = self.get_available_user_ids()
                logger.info(f"Pineconeì—ì„œ ë°œê²¬ëœ ì‚¬ìš©ì ID: {target_user_ids}")
            
            if not target_user_ids:
                logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ìš©ì IDê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "batch_metadata": {
                        "start_time": datetime.now().isoformat(),
                        "error": "ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ìš©ì IDê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "namespace": self.namespace
                    },
                    "batch_summary": {
                        "successful_evaluations": 0,
                        "failed_evaluations": 0
                    }
                }
            
            batch_results = {
                "batch_metadata": {
                    "start_time": datetime.now().isoformat(),
                    "target_user_ids": target_user_ids,
                    "total_users": len(target_user_ids),
                    "namespace": self.namespace,
                    "data_sources": {
                        "weekly_data": f"Pinecone Index: {self.pinecone_index_name}",
                        "team_data": f"MariaDB: {self.db_config['host']}/{self.db_config['database']}"
                    }
                },
                "individual_results": {},
                "batch_summary": {
                    "successful_evaluations": 0,
                    "failed_evaluations": 0
                }
            }
            
            # ê°œë³„ ì‚¬ìš©ì í‰ê°€ ì‹¤í–‰
            for user_id in target_user_ids:
                logger.info(f"ë°°ì¹˜ í‰ê°€ ì§„í–‰ ì¤‘: {user_id}")
                
                try:
                    result = self.execute_single_evaluation(user_id)
                    
                    batch_results["individual_results"][user_id] = result
                    
                    if "error" not in result:
                        batch_results["batch_summary"]["successful_evaluations"] += 1
                    else:
                        batch_results["batch_summary"]["failed_evaluations"] += 1
                        
                except Exception as e:
                    logger.error(f"ì‚¬ìš©ì {user_id} ë°°ì¹˜ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
                    batch_results["individual_results"][user_id] = {
                        "error": str(e),
                        "user_id": user_id
                    }
                    batch_results["batch_summary"]["failed_evaluations"] += 1
            
            batch_results["batch_metadata"]["end_time"] = datetime.now().isoformat()
            
            # ë°°ì¹˜ ê²°ê³¼ ì €ì¥ (reference ì •ë³´ í¬í•¨)
            batch_reference_info = {
                "data_sources": {
                    "weekly_data": "Pinecone Vector DB",
                    "team_criteria": "MariaDB.team_criteria", 
                    "team_goals": "MariaDB.team_goal"
                },
                "pinecone_matches": [],  # ë°°ì¹˜ì—ì„œëŠ” ê°œë³„ ë§¤ì¹˜ ì •ë³´ ì œì™¸
                "documents_found": 0,
                "namespace": self.namespace
            }
            
            batch_output_file = self.save_evaluation_results(
                batch_results,
                batch_reference_info,
                f"batch_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            
            logger.info(f"ë°°ì¹˜ í‰ê°€ ì™„ë£Œ - ì„±ê³µ: {batch_results['batch_summary']['successful_evaluations']}ëª…, "
                       f"ì‹¤íŒ¨: {batch_results['batch_summary']['failed_evaluations']}ëª…")
            
            return batch_results
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            return {
                "batch_metadata": {
                    "start_time": datetime.now().isoformat(),
                    "error": str(e),
                    "namespace": self.namespace
                },
                "batch_summary": {
                    "successful_evaluations": 0,
                    "failed_evaluations": len(target_user_ids) if target_user_ids else 0
                }
            }

    def get_evaluation_history(self) -> List[Dict[str, Any]]:
        """í‰ê°€ ì´ë ¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.evaluation_history

    def get_available_user_ids(self) -> List[str]:
        """Pineconeì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  user_idë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        logger.info("Pineconeì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ user_id ì¡°íšŒ ì‹œì‘")
        
        try:
            # ë¨¼ì € ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
            stats = self.index.describe_index_stats()
            total_vectors = stats.total_vector_count
            print(f"ğŸ” ì¸ë±ìŠ¤ í†µê³„: ì´ {total_vectors}ê°œ ë²¡í„°")
            
            if total_vectors == 0:
                logger.warning("ì¸ë±ìŠ¤ì— ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë°ì´í„° ì¡°íšŒ ì‹œë„
            user_ids = set()
            
            # ë°©ë²• 1: ê¸°ë³¸ ì¿¼ë¦¬ (ë”ë¯¸ ë²¡í„° ì‚¬ìš©)
            dummy_vector = [0.0] * 1024
            
            # ë” ë§ì€ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²ˆ ì¿¼ë¦¬
            for attempt in range(3):
                try:
                    # ë§¤ë²ˆ ë‹¤ë¥¸ ë”ë¯¸ ë²¡í„° ì‚¬ìš©
                    if attempt > 0:
                        dummy_vector = [random.uniform(-1, 1) for _ in range(1024)]
                    
                    query_result = self.index.query(
                        vector=dummy_vector,
                        top_k=min(10000, total_vectors),  # ê°€ëŠ¥í•œ ìµœëŒ€ê°’
                        include_metadata=True
                    )
                    
                    print(f"ğŸ”„ ì‹œë„ {attempt + 1}: {len(query_result.matches)}ê°œ ê²°ê³¼")
                    
                    for match in query_result.matches:
                        if hasattr(match, 'metadata') and match.metadata:
                            # ë‹¤ì–‘í•œ í‚¤ ì´ë¦„ í™•ì¸
                            for key in ['user_id', 'userId', 'USER_ID', 'employee_id', 'emp_id']:
                                if key in match.metadata:
                                    user_ids.add(str(match.metadata[key]))
                                    
                            # ë©”íƒ€ë°ì´í„° í‚¤ ë””ë²„ê¹… (ì²« ë²ˆì§¸ ë§¤ì¹˜ì—ì„œë§Œ)
                            if attempt == 0 and len(user_ids) == 0:
                                print(f"ğŸ” ë©”íƒ€ë°ì´í„° í‚¤ í™•ì¸: {list(match.metadata.keys())}")
                                print(f"ğŸ” ìƒ˜í”Œ ë©”íƒ€ë°ì´í„°: {dict(list(match.metadata.items())[:5])}")
                    
                    if user_ids:
                        break
                        
                except Exception as e:
                    print(f"âŒ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                    continue
            
            # ë°©ë²• 2: ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸ ë° ê²€ìƒ‰
            if not user_ids:
                print("ğŸ”„ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ê²€ìƒ‰ ì‹œë„...")
                try:
                    if hasattr(stats, 'namespaces') and stats.namespaces:
                        for namespace in stats.namespaces.keys():
                            print(f"ğŸ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '{namespace}' ê²€ìƒ‰ ì¤‘...")
                            namespace_result = self.index.query(
                                vector=dummy_vector,
                                top_k=1000,
                                include_metadata=True,
                                namespace=namespace
                            )
                            
                            for match in namespace_result.matches:
                                if hasattr(match, 'metadata') and match.metadata:
                                    for key in ['user_id', 'userId', 'USER_ID', 'employee_id', 'emp_id']:
                                        if key in match.metadata:
                                            user_ids.add(str(match.metadata[key]))
                            
                            if user_ids:
                                print(f"âœ… ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '{namespace}'ì—ì„œ {len(user_ids)}ê°œ user_id ë°œê²¬")
                                break
                except Exception as e:
                    print(f"âŒ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ë°©ë²• 3: fetch API ì‚¬ìš© (IDë¥¼ ì•Œê³  ìˆëŠ” ê²½ìš°)
            if not user_ids:
                print("ğŸ”„ Fetch API ì‹œë„...")
                try:
                    # ì¸ë±ìŠ¤ì—ì„œ ì¼ë¶€ ID ê°€ì ¸ì˜¤ê¸°
                    first_query = self.index.query(
                        vector=dummy_vector,
                        top_k=10,
                        include_metadata=False  # IDë§Œ ê°€ì ¸ì˜¤ê¸°
                    )
                    
                    if first_query.matches:
                        # ì²« ë²ˆì§¸ IDë“¤ë¡œ fetch ì‹œë„
                        ids_to_fetch = [match.id for match in first_query.matches[:5]]
                        fetch_result = self.index.fetch(ids=ids_to_fetch)
                        
                        for id_key, vector_data in fetch_result.vectors.items():
                            if hasattr(vector_data, 'metadata') and vector_data.metadata:
                                for key in ['user_id', 'userId', 'USER_ID', 'employee_id', 'emp_id']:
                                    if key in vector_data.metadata:
                                        user_ids.add(str(vector_data.metadata[key]))
                                        
                                if not user_ids:  # ì²« ë²ˆì§¸ì—ì„œ ë©”íƒ€ë°ì´í„° í‚¤ í™•ì¸
                                    print(f"ğŸ” Fetch ë©”íƒ€ë°ì´í„° í‚¤: {list(vector_data.metadata.keys())}")
                        
                except Exception as e:
                    print(f"âŒ Fetch API ì‹¤íŒ¨: {e}")
            
            available_ids = sorted(list(user_ids))
            
            if available_ids:
                logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ user_id: {available_ids}")
                print(f"âœ… ì´ {len(available_ids)}ëª…ì˜ ì‚¬ìš©ì ë°œê²¬: {available_ids}")
            else:
                logger.warning("user_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë©”íƒ€ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                print("âŒ user_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # ë””ë²„ê¹…ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´ ì¶œë ¥
                try:
                    debug_query = self.index.query(
                        vector=dummy_vector,
                        top_k=1,
                        include_metadata=True
                    )
                    if debug_query.matches:
                        print(f"ğŸ” ë””ë²„ê¹…: ì „ì²´ ë©”íƒ€ë°ì´í„° êµ¬ì¡°")
                        print(f"    {debug_query.matches[0].metadata}")
                except:
                    pass
            
            return available_ids
            
        except Exception as e:
            logger.error(f"user_id ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ ì „ì²´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def debug_pinecone_search(self, user_id: str) -> Dict[str, Any]:
        """Pinecone ê²€ìƒ‰ ë””ë²„ê¹…ì„ ìœ„í•œ í•¨ìˆ˜"""
        logger.info(f"Pinecone ê²€ìƒ‰ ë””ë²„ê¹… - user_id: {user_id}")
        
        try:
            dummy_vector = [0.0] * 1024
            
            # 1. ì „ì²´ ê²€ìƒ‰ (í•„í„° ì—†ìŒ)
            all_results = self.index.query(
                vector=dummy_vector,
                top_k=10,
                include_metadata=True
            )
            
            print(f"ğŸ” ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 10ê°œ):")
            for i, match in enumerate(all_results.matches, 1):
                metadata = match.metadata
                print(f"  {i}. ID: {match.id}")
                print(f"     user_id: {metadata.get('user_id')} (íƒ€ì…: {type(metadata.get('user_id'))})")
                print(f"     organization_id: {metadata.get('organization_id')}")
                print(f"     start_date: {metadata.get('start_date')}")
                print()
            
            # 2. íŠ¹ì • user_idë¡œ í•„í„°ë§
            filtered_results = self.index.query(
                vector=dummy_vector,
                filter={"user_id": str(user_id)},
                top_k=10,
                include_metadata=True
            )
            
            print(f"ğŸ¯ user_id '{user_id}' í•„í„°ë§ ê²°ê³¼:")
            print(f"   ì°¾ì€ ê²°ê³¼ ìˆ˜: {len(filtered_results.matches)}")
            
            for i, match in enumerate(filtered_results.matches, 1):
                metadata = match.metadata
                print(f"  {i}. ID: {match.id}")
                print(f"     user_id: {metadata.get('user_id')}")
                print(f"     done_task: {metadata.get('done_task', '')[:100]}...")
                print()
            
            return {
                "total_results": len(all_results.matches),
                "filtered_results": len(filtered_results.matches),
                "available_user_ids": [str(match.metadata.get('user_id')) for match in all_results.matches if 'user_id' in match.metadata]
            }
            
        except Exception as e:
            logger.error(f"ë””ë²„ê¹… ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}

    def get_evaluation_statistics(self) -> Dict[str, Any]:
        """í‰ê°€ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.evaluation_history:
            return {"total_evaluations": 0}
        
        successful_evaluations = [
            entry for entry in self.evaluation_history 
            if entry.get("status") == "success"
        ]
        
        stats = {
            "total_evaluations": len(self.evaluation_history),
            "successful_evaluations": len(successful_evaluations),
            "failed_evaluations": len(self.evaluation_history) - len(successful_evaluations),
            "latest_evaluation": self.evaluation_history[-1]["timestamp"] if self.evaluation_history else None,
            "evaluated_users": [entry["user_id"] for entry in self.evaluation_history]
        }
        
        return stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ¯ === ìµœì¢… ì£¼ê°„ ë³´ê³ ì„œ í‰ê°€ ì‹œìŠ¤í…œ ===")
    print("ğŸ“‹ Pinecone + MariaDB ê¸°ë°˜ AI í‰ê°€ ì‹œìŠ¤í…œ")
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    openai_key = os.getenv("OPENAI_API_KEY")
    pinecone_key = os.getenv("PINECONE_API_KEY")
    model = os.getenv("OPENAI_MODEL", "gpt-4-turbo")  # ê¸°ë³¸ê°’ ì„¤ì •
    output_path = os.getenv("OUTPUT_PATH", "./output")  # ê¸°ë³¸ê°’ ì„¤ì •
    
    # API í‚¤ ê²€ì¦
    if not openai_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ .env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    if not pinecone_key:
        print("âŒ PINECONE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ .env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    try:
        print(f"\nğŸ¤– ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ëª¨ë¸: {model})")
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        agent = WeeklyReportEvaluationAgent(
            openai_api_key=openai_key,
            pinecone_api_key=pinecone_key,
            model=model,
            output_path=output_path
        )
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        
        # ë‚˜ë¨¸ì§€ ë©”ë‰´ ì½”ë“œëŠ” ë™ì¼...
        # ì‚¬ìš©ì ë©”ë‰´
        while True:
            print(f"\nğŸ¯ === ë©”ì¸ ë©”ë‰´ ===")
            print("1. ëª¨ë“  ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ")
            print("2. ë‹¨ì¼ ì‚¬ìš©ì í‰ê°€")
            print("3. ë°°ì¹˜ í‰ê°€ (ëª¨ë“  ì‚¬ìš©ì)")
            print("4. í‰ê°€ í†µê³„ í™•ì¸")
            print("5. ì¢…ë£Œ")
            
            choice = input("\nì„ íƒí•˜ì„¸ìš” (1-5): ").strip()
            
            if choice == "1":
                print("\nğŸ“‹ ëª¨ë“  ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ ì¤‘...")
                available_users = agent.get_available_user_ids()
                print(f"\nğŸ“Š ì´ {len(available_users)}ëª…ì˜ ì‚¬ìš©ì:")
                for i, user_id in enumerate(available_users, 1):
                    print(f"  {i:2d}. User {user_id}")
            
            elif choice == "2":
                available_users = agent.get_available_user_ids()
                if not available_users:
                    print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                print(f"\nì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ìš©ì: {available_users}")
                user_id = input("í‰ê°€í•  ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if user_id not in available_users:
                    print(f"âŒ ì‚¬ìš©ì ID '{user_id}'ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    continue
                
                print(f"\nğŸš€ ì‚¬ìš©ì {user_id} í‰ê°€ ì‹œì‘...")
                result = agent.execute_single_evaluation(user_id)
                
                # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                if "error" not in result:
                    emp_info = result.get('employee_summary', {}).get('basic_info', {})
                    print(f"\nğŸ‰ === í‰ê°€ ì™„ë£Œ ===")
                    print(f"âœ… ì‚¬ìš©ì: {emp_info.get('name', user_id)}")
                    print(f"ğŸ“Š ì´ í™œë™: {emp_info.get('total_activities', 0)}ê±´")
                    print(f"ğŸ“… í‰ê°€ ê¸°ê°„: {emp_info.get('period', 'N/A')}")
                    
                    activities = result.get('employee_summary', {}).get('activity_categorization', [])
                    print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ í™œë™:")
                    for activity in activities:
                        print(f"   - {activity.get('category', 'Unknown')}: {activity.get('count', 0)}ê±´")
                else:
                    print(f"\nâŒ í‰ê°€ ì‹¤íŒ¨: {result['error']}")
            
            elif choice == "3":
                available_users = agent.get_available_user_ids()
                if not available_users:
                    print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                print(f"\nğŸ“Š ì´ {len(available_users)}ëª…ì˜ ì‚¬ìš©ì ë°°ì¹˜ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
                confirm = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                
                if confirm not in ['y', 'yes']:
                    print("âŒ ë°°ì¹˜ í‰ê°€ë¥¼ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                    continue
                
                print(f"\nğŸš€ ë°°ì¹˜ í‰ê°€ ì‹œì‘...")
                batch_result = agent.execute_batch_evaluation()
                
                # ë°°ì¹˜ ê²°ê³¼ ìš”ì•½
                print(f"\nğŸ‰ === ë°°ì¹˜ í‰ê°€ ê²°ê³¼ ===")
                print(f"ğŸ“Š ì´ ëŒ€ìƒ: {batch_result['batch_metadata']['total_users']}ëª…")
                print(f"âœ… ì„±ê³µ: {batch_result['batch_summary']['successful_evaluations']}ê±´")
                print(f"âŒ ì‹¤íŒ¨: {batch_result['batch_summary']['failed_evaluations']}ê±´")
                print(f"ğŸ“ˆ ì„±ê³µë¥ : {(batch_result['batch_summary']['successful_evaluations']/batch_result['batch_metadata']['total_users']*100):.1f}%")
            
            elif choice == "4":
                stats = agent.get_evaluation_statistics()
                print(f"\nğŸ“ˆ === í‰ê°€ í†µê³„ ===")
                print(f"ì´ í‰ê°€ ìˆ˜í–‰: {stats['total_evaluations']}ê±´")
                print(f"ì„±ê³µí•œ í‰ê°€: {stats['successful_evaluations']}ê±´")
                print(f"ì‹¤íŒ¨í•œ í‰ê°€: {stats['failed_evaluations']}ê±´")
                if stats['total_evaluations'] > 0:
                    print(f"ì„±ê³µë¥ : {(stats['successful_evaluations']/stats['total_evaluations']*100):.1f}%")
                print(f"ìµœê·¼ í‰ê°€: {stats['latest_evaluation']}")
                print(f"í‰ê°€í•œ ì‚¬ìš©ì: {stats['evaluated_users']}")
            
            elif choice == "5":
                print("ğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-5 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
    except Exception as e:
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        print(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        raise


if __name__ == "__main__":
    main()